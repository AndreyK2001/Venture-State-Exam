@inproceedings{del-fishel-2022-cross,
    title = "Cross-lingual Similarity of Multilingual Representations Revisited",
    author = "Del, Maksym  and Fishel, Mark",
    booktitle = "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = nov,
    year = "2022",
    address = "Online only",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.aacl-main.15",
    pages = "185--195",
    abstract = "Related works used indexes like CKA and variants of CCA to measure the similarity of cross-lingual representations in multilingual language models. In this paper, we argue that assumptions of CKA/CCA align poorly with one of the motivating goals of cross-lingual learning analysis, i.e., explaining zero-shot cross-lingual transfer. We highlight what valuable aspects of cross-lingual similarity these indexes fail to capture and provide a motivating case study demonstrating the problem empirically. Then, we introduce Average Neuron-Wise Correlation (ANC) as a straightforward alternative that is exempt from the difficulties of CKA/CCA and is good specifically in a cross-lingual context. Finally, we use ANC to construct evidence that the previously introduced {``}first align, then predict{''} pattern takes place not only in masked language models (MLMs) but also in multilingual models with causal language modeling objectives (CLMs). Moreover, we show that the pattern extends to the \textit{scaled versions} of the MLMs and CLMs (up to 85x original mBERT). Our code is publicly available at https://github.com/TartuNLP/xsim",
}

@article{naous2023readme,
  title={ReadMe++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment},
  author={Naous, Tarek and Ryan, Michael J and Lavrouk, Anton and Chandra, Mohit and Xu, Wei},
  journal={arXiv preprint arXiv:2305.14463},
  year={2023}
}

@inproceedings{muller-etal-2021-first,
    title = "First Align, then Predict: Understanding the Cross-Lingual Ability of Multilingual {BERT}",
    author = "Muller, Benjamin  and Elazar, Yanai  and Sagot, Beno{\^\i}t  and Seddah, Djam{\'e}",
    editor = "Merlo, Paola  and Tiedemann, Jorg  and Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.189",
    doi = "10.18653/v1/2021.eacl-main.189",
    pages = "2214--2231",
    abstract = "Multilingual pretrained language models have demonstrated remarkable zero-shot cross-lingual transfer capabilities. Such transfer emerges by fine-tuning on a task of interest in one language and evaluating on a distinct language, not seen during the fine-tuning. Despite promising results, we still lack a proper understanding of the source of this transfer. Using a novel layer ablation technique and analyses of the model{'}s internal representations, we show that multilingual BERT, a popular multilingual language model, can be viewed as the stacking of two sub-networks: a multilingual encoder followed by a task-specific language-agnostic predictor. While the encoder is crucial for cross-lingual transfer and remains mostly unchanged during fine-tuning, the task predictor has little importance on the transfer and can be reinitialized during fine-tuning. We present extensive experiments with three distinct tasks, seventeen typologically diverse languages and multiple domains to support our hypothesis.",
}

@Book{sharoff23bucc,
  author    = {Sharoff, Serge and Rapp, Reinhard and Zweigenbaum, Pierre},
  title     = {Building and Using Comparable Corpora for Multilingual Natural Language Processing},
  publisher = {Springer Nature},
  year      = {2023},
  series    = {Synthesis Lectures on Human Language Technologies},
  url       = {https://link.springer.com/book/9783031313837}
}

@inproceedings{devlin2019bert,
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle = {Proceedings of NAACL-HLT},
  pages = {4171--4186},
  year = {2019}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI blog},
  volume={1},
  number={8},
  year={2019}
}

@inproceedings{conneau2019unsupervised,
  title = {Unsupervised Cross-lingual Representation Learning at Scale},
  author = {Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzmán, Francisco and Stoyanov, Veselin},
  booktitle = {Proceedings of ACL},
  pages = {8440--8451},
  year = {2020}
}

@inproceedings{vaswani2017attention,
  title = {Attention is All You Need},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Polosukhin, Illia},
  booktitle = {Advances in Neural Information Processing Systems},
  pages = {5998--6008},
  year = {2017}
}

@inproceedings{pires2019multilingual,
  title = {How Multilingual is Multilingual BERT?},
  author = {Pires, Telmo and Schlinger, Eva and Garrette, Dan},
  booktitle = {Proceedings of ACL},
  pages = {4996--5001},
  year = {2019}
}

@inproceedings{muller2021first,
  title = {First Align, then Predict: Understanding the Cross-Lingual Ability of Multilingual BERT},
  author = {Muller, Benjamin and Elazar, Yanai and Sagot, Benoît and Seddah, Djamé},
  booktitle = {Proceedings of ACL},
  year = {2021}
}

@inproceedings{wu2019beto,
  title = {Are All Languages Created Equal in Multilingual BERT?},
  author = {Wu, Shijie and Dredze, Mark},
  booktitle = {Proceedings of the 5th Workshop on Representation Learning for NLP},
  pages = {120--130},
  year = {2020}
}

@inproceedings{hu2020xtreme,
  title = {XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalisation},
  author = {Hu, Jiajun and Ruder, Sebastian and Siddhant, Aditya and Neubig, Graham and Firat, Orhan and Johnson, Melvin},
  booktitle = {Proceedings of ICML},
  pages = {4411--4421},
  year = {2020}
}

@article{hardoon2004canonical,
  title = {Canonical Correlation Analysis: An Overview with Application to Learning Methods},
  author = {Hardoon, David R. and Szedmak, Sandor and Shawe-Taylor, John},
  journal = {Neural Computation},
  volume = {16},
  number = {12},
  pages = {2639--2664},
  year = {2004}
}

@inproceedings{raghu2017svcca,
  title = {SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability},
  author = {Raghu, Maithra and Gilmer, Justin and Yosinski, Jason and Sohl-Dickstein, Jascha},
  booktitle = {Advances in Neural Information Processing Systems},
  pages = {6076--6085},
  year = {2017}
}

@inproceedings{kornblith2019similarity,
  title = {Similarity of Neural Network Representations Revisited},
  author = {Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
  booktitle = {Proceedings of ICML},
  pages = {3519--3529},
  year = {2019}
}

@book{CouncilofEurope-01,
  title     = {Common European Framework of Reference for Languages: Learning, Teaching, Assessment},
  author    = {{Council of Europe}},
  year      = {2001},
  publisher = {Cambridge University Press},
  address   = {Cambridge, UK},
  isbn      = {978-0-521-13088-4},
  url       = {https://www.coe.int/en/web/common-european-framework-reference-languages}
}


@article{abnar2019blackbox,
  title = {Blackbox Meets Blackbox: Representational Similarity and Stability Analysis of Neural Language Models and Brains},
  author = {Abnar, Samira and Zuidema, Willem and Raijmakers, Maartje E. J.},
  journal = {arXiv preprint arXiv:1906.01539},
  year = {2019}
}

@article{lin2021few,
  title = {Few-shot Learning with Multilingual Language Models},
  author = {Lin, Weijia and Ren, Xiang and He, He},
  journal = {arXiv preprint arXiv:2107.06383},
  year = {2021}
}

@article{ponti2019modeling,
  title = {Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing},
  author = {Ponti, Edoardo M. and Vulić, Ivan and Glavaš, Goran and Reichart, Roi and Korhonen, Anna and Henderson, James},
  journal = {Computational Linguistics},
  volume = {45},
  number = {3},
  pages = {559--601},
  year = {2019}
}

@inproceedings{nooralahzadeh2020zero,
  title = {Zero-shot Cross-lingual Transfer with Meta Learning},
  author = {Nooralahzadeh, Farhad and Tutek, Martin and Vulić, Ivan},
  booktitle = {Proceedings of EMNLP},
  pages = {4547--4562},
  year = {2020}
}

@inproceedings{kudo2018sentencepiece,
  title = {SentencePiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing},
  author = {Kudo, Taku and Richardson, John},
  booktitle = {Proceedings of EMNLP},
  pages = {66--71},
  year = {2018}
}

@inproceedings{rust2021good,
  title = {How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models},
  author = {Rust, Phillip and Tuggener, Don and Marelli, Marco and Klinger, Roman},
  booktitle = {Proceedings of EACL},
  pages = {235--249},
  year = {2021}
}

@inproceedings{voita2019analyzing,
  title = {Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned},
  author = {Voita, Elena and Talbot, Jean and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
  booktitle = {Proceedings of ACL},
  pages = {5797--5808},
  year = {2019}
}

@inproceedings{clark2019does,
  title = {What Does BERT Look at? An Analysis of BERT's Attention},
  author = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.},
  booktitle = {Proceedings of ACL},
  pages = {276--286},
  year = {2019}
}

@inproceedings{naous2022readme++,
  title = {README++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment},
  author = {Naous, Tarek and Ryan, Michael J. and Lavrouk, Anton and Chandra, Mohit and Xu, Wei},
  booktitle = {Proceedings of NAACL-HLT},
  year = {2022}
}

@inproceedings{conneau2018xnli,
  title = {XNLI: Evaluating Cross-lingual Sentence Representations},
  author = {Conneau, Alexis and Rinott, Ruty and Lample, Guillaume and Williams, Adina and Bowman, Samuel and Schwenk, Holger and Stoyanov, Veselin},
  booktitle = {Proceedings of EMNLP},
  pages = {2475--2485},
  year = {2018}
}

@inproceedings{yang2019paws,
  title = {PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification},
  author = {Yang, Yinfei and Zhang, Yuan and Tar, Chris and Baldridge, Jason},
  booktitle = {Proceedings of EMNLP},
  pages = {3685--3690},
  year = {2019}
}

@article{radford2018improving,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  journal={OpenAI preprint},
  year={2018},
  url={https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf}
}

@article{brown2020language,
  title={Language Models are Few-Shot Learners},
  author={Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{raffel2020exploring,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@inproceedings{yang2019xlnet,
  title={XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5753--5763},
  year={2019}
}

@article{openai2023gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{chowdhery2022palm,
  title={PaLM: Scaling Language Modeling with Pathways},
  author={Chowdhery, Aakanksha and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@inproceedings{wang2018glue,
  title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  booktitle={Proceedings of the 2018 EMNLP Workshop BlackboxNLP},
  pages={353--355},
  year={2018}
}

@inproceedings{wang2019superglue,
  title={SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  booktitle={Advances in Neural Information Processing Systems},
  volume={32},
  pages={3266--3280},
  year={2019}
}

@article{shoeybi2019megatron,
  title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Rajiv and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@inproceedings{lu2021multimodal,
  title={Pretrained Multimodal Transformers for Multimodal Sequence Learning},
  author={Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={5275--5285},
  year={2021}
}

@inproceedings{bender2021dangers,
  title={On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  author={Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Margaret},
  booktitle={Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  pages={610--623},
  year={2021}
}

@article{gao2020pile,
  title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@article{scao2022bloom,
  title={BLOOM: A 176B-Parameter Open-Access Multilingual Language Model},
  author={Scao, Teven Le and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}

@inproceedings{strubell2019energy,
  title={Energy and Policy Considerations for Deep Learning in NLP},
  author={Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={3645--3650},
  year={2019}
}

@article{schwartz2020green,
  title={Green AI},
  author={Schwartz, Roy and Dodge, Jesse and Smith, Noah A and Etzioni, Oren},
  journal={Communications of the ACM},
  volume={63},
  number={12},
  pages={54--63},
  year={2020}
}

@inproceedings{ganesh2021compressing,
  title={Compressing Large-Scale Transformer-Based Models: A Case Study on BERT},
  author={Ganesh, Ananya and Chen, Tianlong and Zhang, Haocheng and Li, Zhangyang and Wang, Xia},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={3285--3300},
  year={2021}
}

@inproceedings{gehman2020realtoxicityprompts,
  title={RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models},
  author={Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi, Yejin and Smith, Noah A},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},
  pages={3356--3369},
  year={2020}
}

@inproceedings{bolukbasi2016man,
  title={Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings},
  author={Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James Y and Saligrama, Venkatesh and Kalai, Adam T},
  booktitle={Proceedings of the 30th International Conference on Neural Information Processing Systems},
  pages={4349--4357},
  year={2016}
}

@article{weidinger2021ethical,
  title={Ethical and Social Risks of Harm from Language Models},
  author={Weidinger, Laura and other},
  journal={arXiv preprint arXiv:2112.04359},
  year={2021}
}

@article{lipton2018mythos,
  title={The Mythos of Model Interpretability},
  author={Lipton, Zachary C},
  journal={Queue},
  volume={16},
  number={3},
  pages={31--57},
  year={2018}
}

@article{danilevsky2020explainable,
  title={A Survey of the State of Explainable AI for Natural Language Processing},
  author={Danilevsky, Marina and Qian, Kun and Aharonov, Ranit and Katsis, Yannis and Kawas, Ban and Sen, Prithviraj},
  journal={arXiv preprint arXiv:2010.00711},
  year={2020}
}

@article{arrieta2020explainable,
  title={Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI},
  author={Arrieta, Alejandro Barredo and others},
  journal={Information Fusion},
  volume={58},
  pages={82--115},
  year={2020}
}

@inproceedings{zhou2021evaluating,
  title={Evaluating Commonsense in Pre-Trained Language Models},
  author={Zhou, Hao and Ning, Qingkai and Bian, Jiangtao and Wang, Ming and Sun, Jie},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics},
  pages={4146--4159},
  year={2021}
}

@inproceedings{gururangan2020don,
  title={Don't Stop Pretraining: Adapt Language Models to Domains and Tasks},
  author={Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={8342--8360},
  year={2020}
}

@inproceedings{zellers2019defending,
  title={Defending Against Neural Fake News},
  author={Zellers, Rowan and Holtzman, Ari and Rashkin, Hannah and Bisk, Yonatan and Farhadi, Ali and Roesner, Franziska and Choi, Yejin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9051--9062},
  year={2019}
}

@inproceedings{bagdasaryan2019differential,
  title={Differential Privacy Has Disparate Impact on Model Accuracy},
  author={Bagdasaryan, Eugene and Poursaeed, Omid and Shmatikov, Vitaly},
  booktitle={Advances in Neural Information Processing Systems},
  pages={15453--15462},
  year={2019}
}

@article{calo2017artificial,
  title={Artificial Intelligence Policy: A Primer and Roadmap},
  author={Calo, Ryan},
  journal={UC Davis Law Review},
  volume={51},
  pages={399},
  year={2017}
}

@inproceedings{hinton2015distilling,
  title={Distilling the Knowledge in a Neural Network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  booktitle={Proceedings of the NIPS Deep Learning Workshop},
  year={2015}
}

@article{sanh2019distilbert,
  title={DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@article{tay2020efficient,
  title={Efficient Transformers: A Survey},
  author={Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  journal={arXiv preprint arXiv:2009.06732},
  year={2020}
}

@inproceedings{zhao2018gender,
  title={Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods},
  author={Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Ordonez, Vicente and Chang, Kai-Wei},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={15--20},
  year={2018}
}

@article{bender2018data,
  title={Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science},
  author={Bender, Emily M and Friedman, Batya},
  journal={Transactions of the Association for Computational Linguistics},
  volume={6},
  pages={587--604},
  year={2018}
}

@inproceedings{ribeiro2016should,
  title={"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages={1135--1144},
  year={2016}
}

@inproceedings{lauscher2020zero,
  title={From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers},
  author={Lauscher, Anne and Glava{\v{s}}, Goran and Ponzetto, Simone Paolo},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},
  pages={4483--4499},
  year={2020}
}

@inproceedings{wang2020extending,
  title={Extending Multilingual BERT to Low-Resource Languages},
  author={Wang, Wuwei and Kuo, Kevin and Zhu, Ziru and Gimpel, Kevin},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},
  pages={7522--7528},
  year={2020}
}

@article{liao2021improving,
  title={Improving Multilingual Language Models by Retro-fitting Variational Inference and Optimal Transport},
  author={Liao, Renqian and Zoph, Barret and Meijer, Pieter-Jan Kindermans and Vasudevan, Vijay and Le, Quoc V},
  journal={arXiv preprint arXiv:2104.08786},
  year={2021}
}

@inproceedings{ippolito2020automatic,
  title={Automatic Detection of Generated Text is Easiest when Humans are Fooled},
  author={Ippolito, Daphne and Duckworth, Douglas and Callison-Burch, Chris and Eck, Douglas},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={1808--1822},
  year={2020}
}

@article{solaiman2019release,
  title={Release Strategies and the Social Impacts of Language Models},
  author={Solaiman, Irene and Brundage, Miles and Clark, Jack and Askell, Amanda and Herbert-Voss, Ariel and Wu, Jeffrey and Radford, Alec and Krueger, Gretchen and Kim, Jong Wook and Kreps, Sarah and McCain, Molly},
  journal={arXiv preprint arXiv:1908.09203},
  year={2019}
}

@inproceedings{krause2020gedi,
  title={GeDi: Generative Discriminator Guided Sequence Generation},
  author={Krause, Ben and Goel, Kalpesh Krishna and McCann, Bryan},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={14277--14289},
  year={2020}
}

@misc{microsoft2019openai,
  title        = {Microsoft Invests in and Partners with OpenAI to Support Us Building Beneficial AGI},
  author       = {{OpenAI}},
  year         = {2019},
  howpublished = {\url{https://openai.com/blog/microsoft/}},
}

@misc{microsoft2023openai,
  title        = {Microsoft and OpenAI Extend Partnership},
  author       = {{Microsoft}},
  year         = {2023},
  howpublished = {\url{https://blogs.microsoft.com/blog/2023/01/23/microsoft-and-openai-extend-partnership/}},
}

@misc{alphabet2022annualreport,
  title        = {Alphabet Inc. Annual Report 2022},
  author       = {{Alphabet Inc.}},
  year         = {2022},
  howpublished = {\url{https://abc.xyz/investor/static/pdf/20220202_alphabet_10K.pdf}},
}

@misc{meta2023llama,
  title        = {Introducing LLaMA: A foundational, 65-billion-parameter large language model},
  author       = {{Meta AI}},
  year         = {2023},
  howpublished = {\url{https://ai.facebook.com/blog/large-language-model-llama-meta-ai/}},
}

@misc{cbinsights2022ai,
  title        = {Artificial Intelligence Trends To Watch In 2022},
  author       = {{CB Insights}},
  year         = {2022},
  howpublished = {\url{https://www.cbinsights.com/research/report/artificial-intelligence-trends-2022/}},
}

@misc{nvidia2022financial,
  title        = {NVIDIA Announces Financial Results for Fourth Quarter and Fiscal 2022},
  author       = {{NVIDIA}},
  year         = {2022},
  howpublished = {\url{https://nvidianews.nvidia.com/news/nvidia-announces-financial-results-for-fourth-quarter-and-fiscal-2022}},
}

@misc{eu2021horizoneurope,
  title        = {Horizon Europe - The next EU Research \& Innovation Investment Programme (2021–2027)},
  author       = {{European Commission}},
  year         = {2021},
  howpublished = {\url{https://ec.europa.eu/info/horizon-europe_en}},
}